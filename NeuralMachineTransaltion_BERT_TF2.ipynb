{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralMachineTransaltion_BERT_TF2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "urxCUVCGGeWu"
      },
      "source": [
        "This tutorial trains a Transformer model to translate Chinese to English using pre-trained model BERT as encoder in tensorflow 2.0.\n",
        "\n",
        "You will install tensorflow 2.0 first:\n",
        "```bash\n",
        "pip install tensorflow-gpu==2.0.0-beta1\n",
        "pip install tensorflow-datasets\n",
        "pip install bert-for-tf2\n",
        "```\n",
        "\n",
        "And download the BERT chinese pre-trained model:\n",
        "```bash\n",
        "wget https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYFjyC2AorJu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "5dd9e26c-615b-4055-b7ad-e2e2418c5f20"
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0-beta1\n",
        "!pip install tensorflow-datasets\n",
        "!pip install bert-for-tf2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0-beta1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/53/e18c5e7a2263d3581a979645a185804782e59b8e13f42b9c3c3cfb5bb503/tensorflow_gpu-2.0.0b1-cp36-cp36m-manylinux1_x86_64.whl (348.9MB)\n",
            "\u001b[K     |███████████████████████████▏    | 295.7MB 1.4MB/s eta 0:00:38"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrjnOqWropxo"
      },
      "source": [
        "!pip uninstall -y tensorflow\n",
        "!pip install tensorflow==1.14.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJSgYMa0xGm9"
      },
      "source": [
        "!pip install --upgrade six"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uj4xtKOWw1sj"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MeR7cgVx0jW"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sK250-O9x1ho"
      },
      "source": [
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pne5z4cTGeWv"
      },
      "source": [
        "##Prepare the package requirements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ggahts4FGeWv"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "import unicodedata\n",
        "\n",
        "import os\n",
        "from bert import BertModelLayer\n",
        "from bert.loader import StockBertConfig, load_stock_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOEQJQTGu9QO"
      },
      "source": [
        "print(\"GPU Available: \", tf.test.is_gpu_available())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLxQ9MlD8Nmj"
      },
      "source": [
        "if not os.path.exists('chinese_L-12_H-768_A-12'):\n",
        "  !wget https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\n",
        "  !unzip chinese_L-12_H-768_A-12"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tan_kJ7RGeWy"
      },
      "source": [
        "## Add BERT tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFB-GPy2GeWz"
      },
      "source": [
        "def convert_to_unicode(text):\n",
        "    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
        "    if isinstance(text, str):\n",
        "        return text\n",
        "    elif isinstance(text, bytes):\n",
        "        return text.decode(\"utf-8\", \"ignore\")\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "\n",
        "\n",
        "def load_vocab(vocab_file):\n",
        "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
        "    vocab = collections.OrderedDict()\n",
        "    index = 0\n",
        "    with tf.io.gfile.GFile(vocab_file, \"r\") as reader:\n",
        "        while True:\n",
        "            token = convert_to_unicode(reader.readline())\n",
        "            if not token:\n",
        "                break\n",
        "            token = token.strip()\n",
        "            vocab[token] = index\n",
        "            index += 1\n",
        "    return vocab\n",
        "\n",
        "\n",
        "def whitespace_tokenize(text):\n",
        "    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "def convert_by_vocab(vocab, items):\n",
        "    \"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"\n",
        "    output = []\n",
        "    for item in items:\n",
        "        output.append(vocab[item])\n",
        "    return output\n",
        "\n",
        "class FullTokenizer(object):\n",
        "    \"\"\"Runs end-to-end tokenziation.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_file, do_lower_case=True):\n",
        "        self.vocab = load_vocab(vocab_file)\n",
        "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
        "        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
        "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        split_tokens = []\n",
        "        for token in self.basic_tokenizer.tokenize(text):\n",
        "            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
        "                split_tokens.append(sub_token)\n",
        "\n",
        "        return split_tokens\n",
        "    \n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "        return convert_by_vocab(self.vocab, tokens)\n",
        "\n",
        "    def convert_ids_to_tokens(self, ids):\n",
        "        return convert_by_vocab(self.inv_vocab, ids)\n",
        "\n",
        "\n",
        "class BasicTokenizer(object):\n",
        "    \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n",
        "\n",
        "    def __init__(self, do_lower_case=True):\n",
        "        \"\"\"Constructs a BasicTokenizer.\n",
        "    \n",
        "        Args:\n",
        "          do_lower_case: Whether to lower case the input.\n",
        "        \"\"\"\n",
        "        self.do_lower_case = do_lower_case\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenizes a piece of text.\"\"\"\n",
        "        text = convert_to_unicode(text)\n",
        "        text = self._clean_text(text)\n",
        "\n",
        "        # This was added on November 1st, 2018 for the multilingual and Chinese\n",
        "        # models. This is also applied to the English models now, but it doesn't\n",
        "        # matter since the English models were not trained on any Chinese data\n",
        "        # and generally don't have any Chinese data in them (there are Chinese\n",
        "        # characters in the vocabulary because Wikipedia does have some Chinese\n",
        "        # words in the English Wikipedia.).\n",
        "        text = self._tokenize_chinese_chars(text)\n",
        "\n",
        "        orig_tokens = whitespace_tokenize(text)\n",
        "        split_tokens = []\n",
        "        for token in orig_tokens:\n",
        "            if self.do_lower_case:\n",
        "                token = token.lower()\n",
        "                token = self._run_strip_accents(token)\n",
        "            split_tokens.extend(self._run_split_on_punc(token))\n",
        "\n",
        "        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
        "        return output_tokens\n",
        "\n",
        "    def _run_strip_accents(self, text):\n",
        "        \"\"\"Strips accents from a piece of text.\"\"\"\n",
        "        text = unicodedata.normalize(\"NFD\", text)\n",
        "        output = []\n",
        "        for char in text:\n",
        "            cat = unicodedata.category(char)\n",
        "            if cat == \"Mn\":\n",
        "                continue\n",
        "            output.append(char)\n",
        "        return \"\".join(output)\n",
        "\n",
        "    def _run_split_on_punc(self, text):\n",
        "        \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
        "        chars = list(text)\n",
        "        i = 0\n",
        "        start_new_word = True\n",
        "        output = []\n",
        "        while i < len(chars):\n",
        "            char = chars[i]\n",
        "            if _is_punctuation(char):\n",
        "                output.append([char])\n",
        "                start_new_word = True\n",
        "            else:\n",
        "                if start_new_word:\n",
        "                    output.append([])\n",
        "                start_new_word = False\n",
        "                output[-1].append(char)\n",
        "            i += 1\n",
        "\n",
        "        return [\"\".join(x) for x in output]\n",
        "\n",
        "    def _tokenize_chinese_chars(self, text):\n",
        "        \"\"\"Adds whitespace around any CJK character.\"\"\"\n",
        "        output = []\n",
        "        for char in text:\n",
        "            cp = ord(char)\n",
        "            if self._is_chinese_char(cp):\n",
        "                output.append(\" \")\n",
        "                output.append(char)\n",
        "                output.append(\" \")\n",
        "            else:\n",
        "                output.append(char)\n",
        "        return \"\".join(output)\n",
        "\n",
        "    def _is_chinese_char(self, cp):\n",
        "        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n",
        "        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n",
        "        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n",
        "        #\n",
        "        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n",
        "        # despite its name. The modern Korean Hangul alphabet is a different block,\n",
        "        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n",
        "        # space-separated words, so they are not treated specially and handled\n",
        "        # like the all of the other languages.\n",
        "        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n",
        "                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n",
        "                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n",
        "                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n",
        "                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n",
        "                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n",
        "                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n",
        "                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _clean_text(self, text):\n",
        "        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
        "        output = []\n",
        "        for char in text:\n",
        "            cp = ord(char)\n",
        "            if cp == 0 or cp == 0xfffd or _is_control(char):\n",
        "                continue\n",
        "            if _is_whitespace(char):\n",
        "                output.append(\" \")\n",
        "            else:\n",
        "                output.append(char)\n",
        "        return \"\".join(output)\n",
        "\n",
        "\n",
        "class WordpieceTokenizer(object):\n",
        "    \"\"\"Runs WordPiece tokenziation.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200):\n",
        "        self.vocab = vocab\n",
        "        self.unk_token = unk_token\n",
        "        self.max_input_chars_per_word = max_input_chars_per_word\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenizes a piece of text into its word pieces.\n",
        "    \n",
        "        This uses a greedy longest-match-first algorithm to perform tokenization\n",
        "        using the given vocabulary.\n",
        "    \n",
        "        For example:\n",
        "          input = \"unaffable\"\n",
        "          output = [\"un\", \"##aff\", \"##able\"]\n",
        "    \n",
        "        Args:\n",
        "          text: A single token or whitespace separated tokens. This should have\n",
        "            already been passed through `BasicTokenizer.\n",
        "    \n",
        "        Returns:\n",
        "          A list of wordpiece tokens.\n",
        "        \"\"\"\n",
        "\n",
        "        text = convert_to_unicode(text)\n",
        "\n",
        "        output_tokens = []\n",
        "        for token in whitespace_tokenize(text):\n",
        "            chars = list(token)\n",
        "            if len(chars) > self.max_input_chars_per_word:\n",
        "                output_tokens.append(self.unk_token)\n",
        "                continue\n",
        "\n",
        "            is_bad = False\n",
        "            start = 0\n",
        "            sub_tokens = []\n",
        "            while start < len(chars):\n",
        "                end = len(chars)\n",
        "                cur_substr = None\n",
        "                while start < end:\n",
        "                    substr = \"\".join(chars[start:end])\n",
        "                    if start > 0:\n",
        "                        substr = \"##\" + substr\n",
        "                    if substr in self.vocab:\n",
        "                        cur_substr = substr\n",
        "                        break\n",
        "                    end -= 1\n",
        "                if cur_substr is None:\n",
        "                    is_bad = True\n",
        "                    break\n",
        "                sub_tokens.append(cur_substr)\n",
        "                start = end\n",
        "\n",
        "            if is_bad:\n",
        "                output_tokens.append(self.unk_token)\n",
        "            else:\n",
        "                output_tokens.extend(sub_tokens)\n",
        "        return output_tokens\n",
        "\n",
        "def _is_whitespace(char):\n",
        "    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
        "    # \\t, \\n, and \\r are technically contorl characters but we treat them\n",
        "    # as whitespace since they are generally considered as such.\n",
        "    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "        return True\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat == \"Zs\":\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def _is_control(char):\n",
        "    \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
        "    # These are technically control characters but we count them as whitespace\n",
        "    # characters.\n",
        "    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "        return False\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat.startswith(\"C\"):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def _is_punctuation(char):\n",
        "    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
        "    cp = ord(char)\n",
        "    # We treat all non-letter/number ASCII as punctuation.\n",
        "    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
        "    # Punctuation class but we treat them as punctuation anyways, for\n",
        "    # consistency.\n",
        "    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
        "            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
        "        return True\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat.startswith(\"P\"):\n",
        "        return True\n",
        "    return False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1Uwxx4lGeW1"
      },
      "source": [
        "## Setup input pipleline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RQZYo0kGeW2"
      },
      "source": [
        "Use TFDS to load the wmt2019 zh-en translation dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Trj0HUmwGeW3"
      },
      "source": [
        "config = tfds.translate.wmt.WmtConfig(\n",
        "    description=\"WMT 2019 translation task dataset.\",\n",
        "    version=\"0.0.3\",\n",
        "    language_pair=(\"zh\", \"en\"),\n",
        "    subsets={\n",
        "        tfds.Split.TRAIN: [\"newscommentary_v13\"],\n",
        "        tfds.Split.VALIDATION: [\"newsdev2017\"],\n",
        "    }\n",
        ")\n",
        "\n",
        "builder = tfds.builder(\"wmt_translate\", config=config)\n",
        "print(builder.info.splits)\n",
        "#builder.download_and_prepare()\n",
        "builder.download_and_prepare()\n",
        "datasets = builder.as_dataset(as_supervised=True)\n",
        "print('datasets is {}'.format(datasets))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5n8epgrGeW7"
      },
      "source": [
        "train_examples = datasets['train']\n",
        "val_examples = datasets['validation']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lumMBmcKoTZ8"
      },
      "source": [
        "for zh, en in train_examples.take(1):\n",
        "  print(tf.compat.as_text(zh.numpy()))\n",
        "  print(tf.compat.as_text(en.numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3u8SkhL6GeW9"
      },
      "source": [
        "Create a custom subwords tokenizer from the training dataset for the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaaeAPebGeW-"
      },
      "source": [
        "vocab_file = 'vocab_en'\n",
        "if os.path.isfile(vocab_file + '.subwords'):\n",
        "  tokenizer_en = tfds.features.text.SubwordTextEncoder.load_from_file(vocab_file)\n",
        "else: \n",
        "  tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "      (en.numpy() for zh, en in train_examples), target_vocab_size=2 ** 13)\n",
        "  tokenizer_en.save_to_file('vocab_en')\n",
        "  \n",
        "sample_string = 'Transformer is awesome.'\n",
        "tokenized_string = tokenizer_en.encode(sample_string)\n",
        "for ts in tokenized_string:\n",
        "  print ('{} ----> {}'.format(ts, tokenizer_en.decode([ts])))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH6tm4WpGeXD"
      },
      "source": [
        "The encoder uses BERT tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeVWQ0oCGeXE"
      },
      "source": [
        "\n",
        "tokenizer_zh = FullTokenizer(\n",
        "        vocab_file='chinese_L-12_H-768_A-12/vocab.txt', do_lower_case=True)\n",
        "\n",
        "test_tokens = tokenizer_zh.tokenize('埃隆·马斯克(Elon Musk)创建了特斯拉')\n",
        "test_ids = tokenizer_zh.convert_tokens_to_ids(['[CLS]'] + test_tokens + ['[SEP]'])\n",
        "print(test_ids)\n",
        "print(tokenizer_zh.convert_ids_to_tokens(test_ids))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDloKnWoGeXG"
      },
      "source": [
        "Add a start and end token to the input and target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbOUAit_GeXH"
      },
      "source": [
        "MAX_SEQ_LENGTH = 128\n",
        "\n",
        "\n",
        "def encode(zh, en, seq_length=MAX_SEQ_LENGTH):\n",
        "    tokens_zh = tokenizer_zh.tokenize(tf.compat.as_text(zh.numpy()))\n",
        "    lang1 = tokenizer_zh.convert_tokens_to_ids(['[CLS]'] + tokens_zh + ['[SEP]'])\n",
        "    if len(lang1) < seq_length:\n",
        "      lang1 = lang1 + list(np.zeros(seq_length - len(lang1), 'int32'))\n",
        "      \n",
        "    lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
        "        tf.compat.as_text(en.numpy())) + [tokenizer_en.vocab_size + 1]\n",
        "    if len(lang2) < seq_length:\n",
        "      lang2 = lang2 + list(np.zeros(seq_length - len(lang2), 'int32'))\n",
        "      \n",
        "    return lang1, lang2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRClaq3Cw5i-"
      },
      "source": [
        "def filter_max_length(x, y, max_length=MAX_SEQ_LENGTH):\n",
        "  return tf.logical_and(tf.size(x) <= max_length,\n",
        "                        tf.size(y) <= max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNe7zbSkGeXJ"
      },
      "source": [
        "Encode texts into integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifgR-DCoGeXK"
      },
      "source": [
        "BUFFER_SIZE = 50000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_dataset = train_examples.map(\n",
        "    lambda zh, en: tf.py_function(encode, [zh, en], [tf.int32, tf.int32]))\n",
        "train_dataset = train_dataset.filter(filter_max_length)\n",
        "\n",
        "# cache the dataset to memory to get a speedup while reading from it.\n",
        "train_dataset = train_dataset.cache()\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(\n",
        "    BATCH_SIZE, padded_shapes=([-1], [-1]), drop_remainder=True)\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "val_dataset = val_examples.map(\n",
        "    lambda zh, en: tf.py_function(encode, [zh, en], [tf.int32, tf.int32]))\n",
        "val_dataset = val_dataset.filter(filter_max_length)\n",
        "val_dataset = val_dataset.padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DY_pVCZmGeXO"
      },
      "source": [
        "## Positional encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfmeGsmGGeXP"
      },
      "source": [
        "Since this model doesn't contain any recurrence or convolution, positional encoding is added to give the model some information about the relative position of the words in the sentence.\n",
        "\n",
        "The positional encoding vector is added to the embedding vector. Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other. But the embeddings do not encode the relative position of words in a sentence. So after adding the positional encoding, words will be closer to each other based on the similarity of their meaning and their position in the sentence, in the d-dimensional space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_c8ztTc4GeXQ"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQlzAsGEGeXR"
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                            np.arange(d_model)[np.newaxis, :],\n",
        "                            d_model)\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    sines = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    cosines = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
        "\n",
        "    pos_encoding = pos_encoding[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIvZZNLdGeXV"
      },
      "source": [
        "## Masking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LQ2K-KnGeXW"
      },
      "source": [
        "Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value 0 is present: it outputs a 1 at those locations, and a 0 otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrfWBXwSGeXX"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  \n",
        "  # add extra dimensions so that we can add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDBeunerGeXZ"
      },
      "source": [
        "The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.\n",
        "\n",
        "This means that to predict the third word, only the first and second word will be used. Similarly to predict the fourth word, only the first, second and the third word will be used and so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkMb_oR7GeXZ"
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8COn99QiGeXc"
      },
      "source": [
        "## Scaled dot product attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFWhCq4xGeXd"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead) \n",
        "    but it must be broadcastable for addition.\n",
        "    \n",
        "    Args:\n",
        "      q: query shape == (..., seq_len_q, depth)\n",
        "      k: key shape == (..., seq_len_k, depth)\n",
        "      v: value shape == (..., seq_len_v, depth_v)\n",
        "      mask: Float tensor with shape broadcastable \n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "      \n",
        "    Returns:\n",
        "      output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "        # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn1jMs9-GeXg"
      },
      "source": [
        "## Multi-head attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne5w002OGeXh"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention,\n",
        "                                        perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIC9xYPCGeXj"
      },
      "source": [
        "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
        "y = tf.random.uniform((1, 60, 768))  # (batch_size, encoder_sequence, d_model)\n",
        "q = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
        "out, attn = temp_mha(y, k=y, q=q, mask=None)\n",
        "out.shape, attn.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qOdTWaxGeXl"
      },
      "source": [
        "## Point wise feed forward network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lJmb-7DGeXl"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "    ])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uG_HsM0GeXn"
      },
      "source": [
        "## Encoder Layer and Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgoHtRikGeXn"
      },
      "source": [
        "Use bert as encoder. The output shape is (batch_size, input_seq_len, d_model)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFG2C0wkGeXo"
      },
      "source": [
        "\n",
        "def build_encoder(config_file):\n",
        "    with tf.io.gfile.GFile(config_file, \"r\") as reader:\n",
        "        stock_params = StockBertConfig.from_json_string(reader.read())\n",
        "        bert_params = stock_params.to_bert_model_layer_params()\n",
        "\n",
        "    return BertModelLayer.from_params(bert_params, name=\"bert\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQCYcYThGeXr"
      },
      "source": [
        "## Decoder Layer and Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpcebMDZGeXr"
      },
      "source": [
        "Each decoder layer consists of sublayers:\n",
        "\n",
        "1. Masked multi-head attention (with look ahead mask and padding mask)\n",
        "\n",
        "2. Multi-head attention (with padding mask). V (value) and K (key) receive the encoder output as inputs. Q (query) receives the output from the masked multi-head attention sublaye\n",
        "\n",
        "3. Point wise feed forward networks\n",
        "\n",
        "Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is LayerNorm(x + Sublayer(x)). The normalization is done on the d_model (last) axis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wXkK9oRGeXr"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "\n",
        "    def call(self, x, enc_output, training,\n",
        "             look_ahead_mask, padding_mask):\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(\n",
        "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvRhep_YGeXt"
      },
      "source": [
        "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
        "sample_encoder_output = tf.random.uniform((64, 128, 768))\n",
        "\n",
        "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
        "    tf.random.uniform((64, 50, 512)), sample_encoder_output,\n",
        "    False, None, None)\n",
        "\n",
        "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtWp6FbaGeXu"
      },
      "source": [
        "The Decoder consists of: 1. Output Embedding 2. Positional Encoding 3. N decoder layers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlaibr1MGeXv"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "                 rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(target_vocab_size, self.d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
        "                           for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training,\n",
        "             look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                                   look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i + 1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2\n",
        "\n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOZywXPjGeXw"
      },
      "source": [
        "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
        "                         dff=2048, target_vocab_size=8000)\n",
        "\n",
        "output, attn = sample_decoder(tf.random.uniform((64, 26)), \n",
        "                              enc_output=sample_encoder_output, \n",
        "                              training=False, look_ahead_mask=None, \n",
        "                              padding_mask=None)\n",
        "\n",
        "output.shape, attn['decoder_layer2_block2'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsCztpCaGeXy"
      },
      "source": [
        "## Create the Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjJdYaqtGeXy"
      },
      "source": [
        "Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToOkfzUifDgj"
      },
      "source": [
        "class Config(object):\n",
        "  def __init__(self, num_layers, d_model, dff, num_heads):\n",
        "    self.num_layers = num_layers\n",
        "    self.d_model = d_model\n",
        "    self.dff = dff\n",
        "    self.num_heads = num_heads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kG9ZtpNCGeXz"
      },
      "source": [
        "from bert.loader import map_to_stock_variable_name\n",
        "\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, config,\n",
        "               target_vocab_size, \n",
        "               bert_config_file,\n",
        "               bert_training=False, \n",
        "               rate=0.1,\n",
        "               name='transformer'):\n",
        "      super(Transformer, self).__init__(name=name)\n",
        "\n",
        "      self.encoder = build_encoder(config_file=bert_config_file)\n",
        "      self.encoder.trainable = bert_training\n",
        "\n",
        "      self.decoder = Decoder(config.num_layers, config.d_model, \n",
        "                             config.num_heads, config.dff, target_vocab_size, rate)\n",
        "\n",
        "      self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "\n",
        "  def load_stock_weights(self, bert: BertModelLayer, ckpt_file):\n",
        "      assert isinstance(bert, BertModelLayer), \"Expecting a BertModelLayer instance as first argument\"\n",
        "      assert tf.compat.v1.train.checkpoint_exists(ckpt_file), \"Checkpoint does not exist: {}\".format(ckpt_file)\n",
        "      ckpt_reader = tf.train.load_checkpoint(ckpt_file)\n",
        "\n",
        "      bert_prefix = 'transformer/bert'\n",
        "\n",
        "      weights = []\n",
        "      for weight in bert.weights:\n",
        "          stock_name = map_to_stock_variable_name(weight.name, bert_prefix)\n",
        "          if ckpt_reader.has_tensor(stock_name):\n",
        "              value = ckpt_reader.get_tensor(stock_name)\n",
        "              weights.append(value)\n",
        "          else:\n",
        "              raise ValueError(\"No value for:[{}], i.e.:[{}] in:[{}]\".format(\n",
        "                  weight.name, stock_name, ckpt_file))\n",
        "      bert.set_weights(weights)\n",
        "      print(\"Done loading {} BERT weights from: {} into {} (prefix:{})\".format(\n",
        "          len(weights), ckpt_file, bert, bert_prefix))\n",
        "\n",
        "  def restore_encoder(self, bert_ckpt_file):\n",
        "      # loading the original pre-trained weights into the BERT layer:\n",
        "      self.load_stock_weights(self.encoder, bert_ckpt_file)\n",
        "\n",
        "  def call(self, inp, tar, training, look_ahead_mask, dec_padding_mask):\n",
        "      enc_output = self.encoder(inp, training=self.encoder.trainable)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "      # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "      dec_output, attention_weights = self.decoder(\n",
        "          tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "      final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "      return final_output, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSdmgqGMGeX2"
      },
      "source": [
        "## Set hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2qMwHTYGeX2"
      },
      "source": [
        "To keep this example small and relatively fast, the values for num_layers, d_model, and dff have been reduced."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJKlwYeuGeX3"
      },
      "source": [
        "target_vocab_size = tokenizer_en.vocab_size + 2\n",
        "dropout_rate = 0.1\n",
        "config = Config(num_layers=6, d_model=256, dff=1024, num_heads=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fXdBCXle0JB"
      },
      "source": [
        "MODEL_DIR = \"chinese_L-12_H-768_A-12\"\n",
        "bert_config_file = os.path.join(MODEL_DIR, \"bert_config.json\")\n",
        "bert_ckpt_file = os.path.join(MODEL_DIR, \"bert_model.ckpt\")\n",
        "\n",
        "transformer = Transformer(config=config,\n",
        "                          target_vocab_size=target_vocab_size,\n",
        "                          bert_config_file=bert_config_file)\n",
        "  \n",
        "inp = tf.random.uniform((BATCH_SIZE, MAX_SEQ_LENGTH))\n",
        "tar_inp = tf.random.uniform((BATCH_SIZE, MAX_SEQ_LENGTH))\n",
        "fn_out, _ = transformer(inp, tar_inp, \n",
        "                        True,\n",
        "                        look_ahead_mask=None,\n",
        "                        dec_padding_mask=None)\n",
        "print(tar_inp.shape) # (batch_size, tar_seq_len) \n",
        "print(fn_out.shape)  # (batch_size, tar_seq_len, target_vocab_size) \n",
        "\n",
        "# init bert pre-trained weights\n",
        "transformer.restore_encoder(bert_ckpt_file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGAWc7sJQ-C4"
      },
      "source": [
        "transformer.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlT1XVQ7GeX4"
      },
      "source": [
        "## Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFeTO3p2GeX5"
      },
      "source": [
        "Use the Adam optimizer with a custom learning rate scheduler according to the formula in the paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyJQLgNEGeX5"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZV-MS99GeX7"
      },
      "source": [
        "learning_rate = CustomSchedule(config.d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
        "                                     epsilon=1e-9)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoeoZUMUGeX9"
      },
      "source": [
        "temp_learning_rate_schedule = CustomSchedule(config.d_model)\n",
        "\n",
        "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBcj8bbeGeX-"
      },
      "source": [
        "## Loss and metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRsfA0--GeX_"
      },
      "source": [
        "Since the target sequences are padded, it is important to apply a padding mask when calculating the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rVFjCHhGeX_"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaTrk5guGeYA"
      },
      "source": [
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9qHuK8oGeYC"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdCj8MwqGeYH"
      },
      "source": [
        "## Training and checkpointing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC8JvcInGeYK"
      },
      "source": [
        "Create the checkpoint path and the checkpoint manager. This will be used to save checkpoints every n epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECbAGB3CGeYK"
      },
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print('Latest checkpoint restored!!')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUIzaBHFGeYM"
      },
      "source": [
        "During training this example uses teacher-forcing (like in the text generation tutorial). Teacher forcing is passing the true output to the next time step regardless of what the model predicts at the current time step.\n",
        "\n",
        "As the transformer predicts each word, self-attention allows it to look at the previous words in the input sequence to better predict the next word.\n",
        "\n",
        "To prevent the model from peaking at the expected output the model uses a look-ahead mask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAsfFtAB1G97"
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by \n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return combined_mask, dec_padding_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwZrRyLVGeYQ"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(inp, tar_inp, \n",
        "                                     True,\n",
        "                                     combined_mask,\n",
        "                                     dec_padding_mask)\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(tar_real, predictions)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocUsGWOIGeYR"
      },
      "source": [
        "Chinese is used as the input language and English is the target language."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh7PlNI7GeYS"
      },
      "source": [
        "\n",
        "EPOCHS = 4\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "\n",
        "    # inp -> chinese, tar -> english\n",
        "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "        train_step(inp, tar)\n",
        "\n",
        "        if batch % 500 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "                epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print('Saving checkpoint for epoch {} at {}'.format(epoch + 1,\n",
        "                                                            ckpt_save_path))\n",
        "\n",
        "    print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1,\n",
        "                                                        train_loss.result(),\n",
        "                                                        train_accuracy.result()))\n",
        "\n",
        "    print('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrA-OoDHGeYT"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzz2_QpaGeYT"
      },
      "source": [
        "The following steps are used for evaluation:\n",
        "\n",
        "- Encode the input sentence using the Portuguese tokenizer (tokenizer_pt). Moreover, add the start and end token so the input is equivalent to what the model is trained with. This is the encoder input.\n",
        "\n",
        "- The decoder input is the start token == tokenizer_en.vocab_size.\n",
        "\n",
        "- Calculate the padding masks and the look ahead masks.\n",
        "\n",
        "- The decoder then outputs the predictions by looking at the encoder output and its own output (self-attention).\n",
        "\n",
        "- Select the last word and calculate the argmax of that.\n",
        "\n",
        "- Concatentate the predicted word to the decoder input as pass it to the decoder.\n",
        "\n",
        "- In this approach, the decoder predicts the next word based on the previous words it predicted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz1yAT3WGeYU"
      },
      "source": [
        "def encode_zh(zh):\n",
        "    tokens_zh = tokenizer_zh.tokenize(zh)\n",
        "    lang1 = tokenizer_zh.convert_tokens_to_ids(['[CLS]'] + tokens_zh + ['[SEP]'])\n",
        "\n",
        "    return lang1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgP8UyKo5xVV"
      },
      "source": [
        "sentence_ids = encode_zh(\"我爱你啊\")\n",
        "print(tokenizer_zh.convert_ids_to_tokens(sentence_ids))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0xE9GOVGeYV"
      },
      "source": [
        "def evaluate(transformer, inp_sentence):\n",
        "    # normalize input sentence\n",
        "    inp_sentence = encode_zh(inp_sentence)\n",
        "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "\n",
        "    # as the target is english, the first word to the transformer should be the\n",
        "    # english start token.\n",
        "    decoder_input = [tokenizer_en.vocab_size]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "    for i in range(MAX_SEQ_LENGTH):\n",
        "        combined_mask, dec_padding_mask = create_masks(\n",
        "            encoder_input, output)\n",
        "\n",
        "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        predictions, attention_weights = transformer(encoder_input,\n",
        "                                                     output,\n",
        "                                                     False,\n",
        "                                                     combined_mask,\n",
        "                                                     dec_padding_mask)\n",
        "\n",
        "        # select the last word from the seq_len dimension\n",
        "        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        # return the result if the predicted_id is equal to the end token\n",
        "        if tf.equal(predicted_id, tokenizer_en.vocab_size + 1):\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "        # concatentate the predicted_id to the output which is given to the decoder\n",
        "        # as its input.\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5X7F9WgGeYW"
      },
      "source": [
        "def plot_attention_weights(attention, sentence, result, layer):\n",
        "    fig = plt.figure(figsize=(16, 8))\n",
        "\n",
        "    sentence_ids = encode_zh(sentence)\n",
        "\n",
        "    attention = tf.squeeze(attention[layer], axis=0)\n",
        "\n",
        "    for head in range(attention.shape[0]):\n",
        "        ax = fig.add_subplot(2, 4, head + 1)\n",
        "\n",
        "        # plot the attention weights\n",
        "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
        "\n",
        "        fontdict = {'fontsize': 10, 'family' : 'DFKai-SB'}\n",
        "\n",
        "        ax.set_xticks(range(len(sentence_ids)))\n",
        "        ax.set_yticks(range(len(result)))\n",
        "\n",
        "        ax.set_ylim(len(result) - 1.5, -0.5)\n",
        "\n",
        "        ax.set_xticklabels(\n",
        "            tokenizer_zh.convert_ids_to_tokens(sentence_ids),\n",
        "            fontdict=fontdict, rotation=90)\n",
        "\n",
        "        ax.set_yticklabels([tokenizer_en.decode([i]) for i in result\n",
        "                            if i < tokenizer_en.vocab_size],\n",
        "                           fontdict=fontdict)\n",
        "\n",
        "        ax.set_xlabel('Head {}'.format(head + 1))\n",
        "\n",
        "        \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIE5ZFM-GeYZ"
      },
      "source": [
        "def translate(transformer, sentence, plot=''):\n",
        "    result, attention_weights = evaluate(transformer, sentence)\n",
        "\n",
        "    predicted_sentence = tokenizer_en.decode([i for i in result\n",
        "                                              if i < tokenizer_en.vocab_size])\n",
        "\n",
        "    print('Input: {}'.format(sentence))\n",
        "    print('Predicted translation: {}'.format(predicted_sentence))\n",
        "\n",
        "    if plot:\n",
        "        plot_attention_weights(attention_weights, sentence, result, plot)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqRUvh4q1p89"
      },
      "source": [
        "translate(transformer, '虽然继承了祖荫，但朴槿惠已经证明了自己是个机敏而老练的政治家——她历经20年才爬上韩国大国家党最高领导层并成为全国知名人物。')\n",
        "print('Real translation: While Park derives some of her power from her family pedigree, she has proven to be an astute and seasoned politician –&nbsp;one who climbed the Grand National Party’s leadership ladder over the last two decades to emerge as a national figure.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-51tfRT4NAt"
      },
      "source": [
        "translate(transformer, \"我爱你是一件幸福的事情。\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9luCwHewWjW"
      },
      "source": [
        "## Save weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyi3Tndv9ov_"
      },
      "source": [
        "transformer.save_weights('bert_nmt_ckpt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHOHMxSoe8bh"
      },
      "source": [
        "\n",
        "new_transformer = Transformer(config=config,\n",
        "                          target_vocab_size=target_vocab_size,\n",
        "                          bert_config_file=bert_config_file)\n",
        "  \n",
        "fn_out, _ = new_transformer(inp, tar_inp, \n",
        "                        True,\n",
        "                        look_ahead_mask=None,\n",
        "                        dec_padding_mask=None)\n",
        "new_transformer.load_weights('bert_nmt_ckpt')\n",
        "\n",
        "translate(new_transformer, '我爱你')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}